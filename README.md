# 法务RAG智能助手 - 基于Ollama+Chroma+Streamlit
一个轻量、本地化部署的法务领域RAG（检索增强生成）问答系统，支持**PDF法务知识库解析**、**本地向量库存储**、**Ollama大模型对接**，提供Streamlit可视化前端和命令行两种交互方式，全程离线运行，保证法务数据隐私性。

## 🌟 核心特性
- 📄 **多PDF批量解析**：支持单/多PDF文件、文件夹批量解析，自动过滤图片型/加密无文本PDF
- ⚖️ **法务文本专属切分**：针对中文法条、规章的语义特点定制文本切分策略，保证法条完整性
- 💾 **本地向量库持久化**：基于Chroma构建本地向量库，首次解析后无需重复处理PDF，直接加载
- 🤖 **Ollama无缝对接**：支持任意Ollama本地模型，默认适配超轻量`qwen2.5:0.5b`（极省内存）
- 🌐 **双端交互**：自带Streamlit可视化前端（展示回答+检索原文）、命令行交互（快速调试）
- 📊 **检索溯源**：回答附带知识库原文片段，可追溯法律依据，避免大模型编造信息
- 🛠️ **极简配置**：仅需修改2个核心路径，其余参数已做法务场景最优配置

## 📋 环境要求
### 基础环境
- Python 3.9+（推荐3.10，兼容Anaconda虚拟环境）
- 操作系统：Windows/Linux/macOS（Windows亲测可用）
- 内存：≥4GB（运行`qwen2.5:0.5b+bge-m3`，8GB以上体验更佳）
- 硬盘：≥10GB（用于存储PDF、向量库、Ollama模型）

### 必须依赖
1. **Ollama**：本地大模型运行环境，[下载地址](https://ollama.com/)
2. **核心Python库**：langchain、chroma、streamlit等（见下方安装命令）

## 🚀 快速开始
### 步骤1：安装Ollama并拉取必要模型
1. 安装Ollama后，启动Ollama服务（后台静默运行，无需额外操作）
2. 打开终端（PowerShell/CMD/Terminal），拉取**嵌入模型**和**对话模型**（命令行执行）：
```bash
# 嵌入模型（必须，用于文本向量化）
ollama pull bge-m3
# 对话模型（默认使用，超轻量极省内存）
ollama pull qwen2.5:0.5b
# 可选：内存充足可拉取更大模型，需同步修改代码中OLLAMA_MODEL参数
ollama pull qwen2.5:1.5b
ollama pull qwen2.5:3b
```

### 步骤2：下载项目代码
1. 新建项目文件夹，将本脚本保存为`1.py`
2. 在项目文件夹下创建**法务PDF知识库文件夹**：`考核素材_法务文档`，并将所有法务PDF放入该文件夹

### 步骤3：安装Python依赖
1. 建议创建Anaconda虚拟环境（避免依赖冲突）：
```bash
# 创建虚拟环境（命名为legal-rag，Python3.10）
conda create -n legal-rag python=3.10 -y
# 激活虚拟环境
conda activate legal-rag
```
2. 安装核心依赖（清华源高速下载）：
```bash
pip install langchain==0.0.350 langchain-chroma langchain-ollama langchain-core langchain-text-splitters PyPDF2 streamlit -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 步骤4：启动系统
#### 方式1：可视化前端（推荐，带界面/检索溯源）
项目目录下执行命令，自动调起浏览器打开前端页面：
```bash
streamlit run 1.py
```
#### 方式2：命令行交互（快速调试/无界面需求）
项目目录下执行命令，直接在终端提问：
```bash
python 1.py --cli
```

## ⚙️ 极简配置（仅需修改这2处！）
脚本顶部`===================== 极简配置项 =====================`区域，仅需根据自身路径修改，其余参数默认适配法务场景：
```python
# 1. 本地法务PDF知识库路径（已默认创建考核素材_法务文档文件夹，无需修改，直接放PDF即可）
PDF_PATH = os.path.join(_SCRIPT_DIR, "考核素材_法务文档")
# 2. 本地Chroma向量库存储路径（首次自动创建，无需手动操作）
CHROMA_DB_PATH = os.path.join(_SCRIPT_DIR, "chroma_legal_db")
```

### 可选参数调整（根据自身硬件/需求）
```python
# Ollama对话模型：内存不足用qwen2.5:0.5b，内存充足可升级为qwen2.5:1.5b/3b/7b
OLLAMA_MODEL = "qwen2.5:0.5b"
# 检索最相关的知识库片段数：默认3个，可根据问题复杂度调整为2/4
RETRIEVE_TOP_K = 3
# 回答温度：0.1为极低温度，保证法务回答准确、无编造，禁止修改为高值
temperature=0.1
```

## 📁 项目目录结构
```
├── 1.py                # 主程序脚本（核心文件）
├── 考核素材_法务文档/   # 法务PDF知识库文件夹（放入所有PDF）
└── chroma_legal_db/    # 本地Chroma向量库（首次运行自动创建，持久化存储）
```

## 📖 使用说明
### 前端使用
1. 启动后浏览器自动打开`http://localhost:8501`，等待知识库和模型加载完成（首次加载因解析PDF/创建向量库稍慢，后续秒开）
2. 文本框输入法律问题（例如：集成电路产业和软件产业是什么产业的核心？）
3. 点击「提交提问」，等待生成回答，支持**展开查看检索到的知识库原文**
4. 回答失败时会给出针对性提示（如内存不足、问题为空等）

### 命令行使用
1. 启动后输入法律问题，直接回车即可获取回答
2. 回答包含「专业回答」和「检索到的知识库原文」两部分
3. 输入`q/quit/退出`即可关闭系统
4. 问题为空时会提示重新输入，避免无效请求

### 知识库更新
1. 向`考核素材_法务文档`文件夹添加/删除/修改PDF文件
2. **删除旧向量库**：直接删除项目目录下的`chroma_legal_db`文件夹
3. 重新启动系统，自动重新解析所有PDF并创建新的向量库

## ❌ 常见问题与解决方案
### 问题1：回答生成失败：llama runner process has terminated: error loading model: unable to allocate CPU buffer
**原因**：内存不足，Ollama加载模型时无法分配足够CPU缓存
**解决方案**：
1. 关闭其他高内存程序（Docker、浏览器多标签、Anaconda、Python等）
2. 任务管理器结束`ollama.exe`进程，重新启动Ollama服务
3. 将`OLLAMA_MODEL`改为更小的模型（如默认的`qwen2.5:0.5b`）
4. 执行`ollama stop`停止所有运行中的Ollama模型，再重新启动系统

### 问题2：解析PDF失败/警告：XXX 无可提取文本
**原因**：PDF为图片型PDF（扫描件）、加密PDF或无文本内容
**解决方案**：
1. 确认PDF为可编辑的文本型PDF，而非扫描件
2. 解密加密PDF后重新放入知识库文件夹
3. 忽略警告，系统会自动过滤该PDF，不影响其他文件解析

### 问题3：启动时提示ModuleNotFoundError: No module named 'xxx'
**原因**：未安装对应依赖/依赖版本不兼容/未激活虚拟环境
**解决方案**：
1. 确认已激活创建的虚拟环境（命令行开头显示`(legal-rag)`）
2. 重新执行步骤3的依赖安装命令，确保所有库安装完成
3. 若仍报错，单独安装缺失的库：`pip install 缺失的库名 -i 清华源`

### 问题4：Streamlit启动失败/页面无法打开
**原因**：端口被占用/Streamlit未安装/Python环境错误
**解决方案**：
1. 确认已安装Streamlit：`pip show streamlit`
2. 更换启动端口：`streamlit run 1.py --server.port 8502`
3. 直接执行`python 1.py`，脚本会自动调起Streamlit

### 问题5：Ollama模型拉取失败/速度慢
**原因**：网络问题，Ollama官方源访问不稳定
**解决方案**：
1. 更换网络（如手机热点）重新拉取
2. 配置Ollama国内镜像源（参考Ollama国内镜像教程）

## 🧰 技术栈
| 模块         | 技术选型                  | 作用                     |
|--------------|---------------------------|--------------------------|
| 文档解析     | PyPDF2                    | 解析PDF提取文本          |
| 文本切分     | langchain-text-splitters  | 法务文本专属切分         |
| 向量化       | OllamaEmbeddings(bge-m3)  | 文本转为向量表示         |
| 向量库       | Chroma                    | 本地向量存储/相似性检索  |
| 大模型交互   | ChatOllama                | 对接本地Ollama大模型     |
| 前端展示     | Streamlit                 | 可视化问答界面/检索溯源  |
| 核心框架     | LangChain                 | 整合RAG全流程            |

## 📌 注意事项
1. **本地化运行**：本系统全程离线运行，无需联网，所有PDF、向量库、模型均存储在本地，保证法务数据隐私性
2. **模型拉取**：首次使用需拉取`bge-m3`和`qwen2.5:0.5b`模型，后续无需重复拉取
3. **向量库持久化**：`chroma_legal_db`文件夹为向量库核心文件，删除后会重新解析所有PDF，请勿随意删除
4. **法务专业性**：本系统为技术演示，回答仅作参考，不构成正式法律意见，实际法律问题请咨询专业律师
5. **硬件适配**：超轻量模型`qwen2.5:0.5b`可在4GB内存电脑运行，更大模型需对应更高内存/显存

## 📄 许可证
本项目为开源学习项目，仅供个人/企业内部法务场景技术研究使用，禁止用于商业盈利目的。

## 📞 问题反馈
若运行过程中遇到未提及的问题，可检查：
1. Ollama服务是否正常运行（`ollama list`可显示模型即正常）
2. 虚拟环境是否激活、依赖是否全部安装
3. PDF文件是否为文本型、无加密
4. 内存是否充足，无其他程序抢占资源

---
**版本**：V1.0  
**适配场景**：本地化法务知识库问答、企业内部法律咨询辅助、法律专业学生学习工具  
**核心优势**：零配置、易部署、本地化、可溯源
